{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_FOLDER= '/content/drive/MyDrive/Dataset'\n",
    "TRAIN_SAMPLE_FOLDER = '/content/drive/MyDrive/Dataset/train_sample_videos'\n",
    "TEST_FOLDER = '/content/drive/MyDrive/Dataset/test_videos'\n",
    "\n",
    "print(f\"train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}\")\n",
    "print(f\"test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "train_sample_metadata = pd.read_json('/content/drive/MyDrive/Dataset/train_sample_videos/metadata.json').T\n",
    "train_sample_metadata.head()\n",
    "\n",
    "f_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].sample(5).index)\n",
    "f_train_sample_video\n",
    "\n",
    "\n",
    "def capture_image_from_video(video_path):\n",
    "    capture_image = cv2.VideoCapture(video_path)\n",
    "    ret, frame = capture_image.read()\n",
    "    fig = plt.figure(figsize =(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(frame)\n",
    "\n",
    "for video_file in f_train_sample_video:\n",
    "    capture_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))\n",
    "\n",
    "r_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='REAL'].sample(5).index)\n",
    "r_train_sample_video\n",
    "\n",
    "for video_file in r_train_sample_video:\n",
    "    capture_image_from_video(os.path.join(DATA_FOLDER,TRAIN_SAMPLE_FOLDER,video_file))\n",
    "\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y,x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y :start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(img_size, img_size)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while 1:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "            \n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "      cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def pretrain_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "    weights = \"imagenet\",\n",
    "    include_top=False,\n",
    "    pooling=\"avg\",\n",
    "    input_shape = (img_size,img_size,3)\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "    \n",
    "    inputs = keras.Input((img_size,img_size,3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "    \n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "feature_extractor = pretrain_feature_extractor()\n",
    "\n",
    "\n",
    "def prepare_all_videos(df, root_dir): #df是train_sample_metadata->json的split\n",
    "    num_samples = len(df)\n",
    "    video_paths = list(df.index)\n",
    "    labels = df[\"label\"].values\n",
    "    labels = np.array(labels=='FAKE').astype(np.int)\n",
    "    \n",
    "    frame_masks = np.zeros(shape=(num_samples, max_seq_length), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, max_seq_length, num_features), dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        temp_frame_mask = np.zeros(shape=(1, max_seq_length,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype=\"float32\")\n",
    "        \n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(max_seq_length, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] =feature_extractor.predict(batch[None, j, :])\n",
    "            temp_frame_mask[i, :length] =1 # 1 = not masked, 0 = masked\n",
    "        \n",
    "        frame_features[idx,] =temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] =temp_frame_mask.squeeze()\n",
    "    \n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_set , Test_set = train_test_split(train_sample_metadata, test_size=0.1,random_state=42,\n",
    "                                       stratify=train_sample_metadata['label'])\n",
    "print(Train_set.shape, Test_set.shape)\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(Train_set, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(Test_set, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set:{train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set:{train_data[1].shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
